{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Wk8: Text Processing as Unstructured Data\n",
    "\n",
    "## EXERCISE: SMS spam filtering with naive Bayes\n",
    "\n",
    "[Adapted from http://radimrehurek.com/data_science_python/.]\n",
    "\n",
    "Other references:\n",
    "* http://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
    "* http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "* https://gist.github.com/zacstewart/5978000\n",
    "\n",
    "Other options:\n",
    "* http://scikit-learn.org/stable/datasets/#the-20-newsgroups-text-dataset\n",
    "* http://scikit-learn.org/stable/datasets/#rcv1-dataset\n",
    "\n",
    "### Download data from UCI ML data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "DATA_URI = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "DATA_DIR = 'data'\n",
    "ARCHIVE_NAME = 'smsspamcollection.zip'\n",
    "FILE_NAME = 'SMSSpamCollection'\n",
    "\n",
    "# set up paths (in portable, OS-agnostic way)\n",
    "local_archive_path = os.path.join(DATA_DIR, ARCHIVE_NAME)\n",
    "local_file_path = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "# set up local data directory\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# save file from DATA_URI to local_path\n",
    "urllib.request.urlretrieve(DATA_URI, local_archive_path)\n",
    "\n",
    "# extract content from archive\n",
    "z = zipfile.ZipFile(local_archive_path, 'r')\n",
    "z.extractall(DATA_DIR)\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and profile data using Pandas\n",
    "\n",
    "Pandas provides tools that streamline some of the data analysis and visualisation work we've done in previous exercises. \n",
    "\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) is the most commonly used data structure in Pandas. It is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table.\n",
    "\n",
    "Let's use it now to read and profile our spam data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas\n",
    "\n",
    "messages = pandas.read_csv(local_file_path, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "print(messages)\n",
    "\n",
    "# view aggregate statistics\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Feature Extraction\n",
    "\n",
    "- It's easy for humans to see the patterns that distinguish ham and spam messages. What kind of features could we feed to a machine learning algorithm?\n",
    "\n",
    "Feeding raw text data (e.g., \"Sorry, I'll call later\") to a machine learning algorithm would not be very useful. As a first step let's assume that there is a systematic difference between the words used for spam and ham.\n",
    "\n",
    "`scikit-learn` includes [several functions for creating feature vectors from text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). These first tokenise -- split strings into words (e.g., \"Sorry\", \"I'll\", \"call\", \"later\").\n",
    "\n",
    "Then then create feature vectors where indices correspond to a specific word and values correspond to the frequency of the corresponding word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# First split in to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages.message, messages.label, test_size=0.30,\n",
    "                                                    random_state=5) # so we get the same results\n",
    "\n",
    "# Fit and transform training to feature vectors of words\n",
    "v = CountVectorizer(binary=True, lowercase=False)\n",
    "X_train_bin = v.fit_transform(X_train)\n",
    "print(X_train_bin.shape)\n",
    "\n",
    "# scikit-learn vectorisers produce sparse array representations.\n",
    "# These store only observed features for each instance instead of a complete vector.\n",
    "# They're great for working with text data, but require a bit of work to inspect.\n",
    "\n",
    "# View feature names\n",
    "print('Feature names:\\n', v.get_feature_names()[:10])\n",
    "\n",
    "# View complete feature vector for row 0 as a list of words\n",
    "def iter_features(X, row, names):\n",
    "    for i in X[row].indices:\n",
    "        yield names[i]\n",
    "print('\\nFeature words for row 0:\\n', list(iter_features(X_train_bin, 0, v.get_feature_names())))\n",
    "\n",
    "# Compare to original data\n",
    "print('\\nOriginal string for row0:\\n', X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipelines and choose parameters\n",
    "\n",
    "`scikit-learn` includes pipeline functionality that makes it possible to specify and optimise a sequence of actions.\n",
    "\n",
    "Let's see whether the results in [Wang and Manning (2012)](http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) hold for the SMS spam data we're using here.\n",
    "\n",
    "Specifically we will compare multinomial naive Bayes to support vector machine with a degree-2 polynomial kernel. We will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline for multinomial naive Bayes\n",
    "mnb = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', MultinomialNB())\n",
    "               ])\n",
    "\n",
    "# Pipeline for polynomial support vector machine\n",
    "svm = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(C=0.1, penalty='l2'))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = [{'vect__binary': [True],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              },\n",
    "              {'vect__binary': [False],\n",
    "               'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': [True, False]\n",
    "              }\n",
    "             ]\n",
    "\n",
    "# Find best parameters for MNB and SVM\n",
    "gs_mnb = GridSearchCV(mnb, param_grid)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print('\\nMNB best params:\\n', gs_mnb.best_params_)\n",
    "\n",
    "gs_svm = GridSearchCV(svm, param_grid)\n",
    "gs_svm.fit(X_train, y_train)\n",
    "print('\\nSVM best params:\\n', gs_svm.best_params_)\n",
    "\n",
    "# Print accuracy\n",
    "print('\\nMNB test result:\\n', classification_report(y_test, gs_mnb.predict(X_test)))\n",
    "print('\\nSVM test result:\\n', classification_report(y_test, gs_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Handling class imbalance with SVM\n",
    "\n",
    "- Which is better? Do we care about overall performance or just one of our classes? How does this compare to Wang and Manning's result?\n",
    "- The `fit()` method for `LinearSVC` includes the `class_weight` parameter with can help deal with imbalanced data. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Train a new SVM model with the best grid search parameters and `class_weight='balanced'`. Is this result better?\n",
    "- Is there a more appropriate scoring function we could pass to `GridSearchCV` (using `sklearn.metrics.make_scorer`)? Does this give different MNB or SVM results for the parameter grid above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EXERCISE: Natural language parsing with SpaCy\n",
    "\n",
    "[Adapted from https://nicschrading.com/project/Intro-to-NLP-with-spaCy/.]\n",
    "\n",
    "### Load English parser\n",
    "\n",
    "`parser = English()` will initialise the models for core English language processing.\n",
    "\n",
    "We'll use some text from the first chapter of Alice in Wonderland, which is available from Project Gutenberg.\n",
    "\n",
    "Perhaps enjoy reading while waiting for the models to load (may take a minute or two). ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up spaCy\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "# Our text data from Ch1 of Alice in Wonderland\n",
    "alice = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the\n",
    "bank, and of having nothing to do. Once or twice she had peeped into the\n",
    "book her sister was reading, but it had no pictures or conversations in\n",
    "it, \"and what is the use of a book,\" thought Alice, \"without pictures or\n",
    "conversations?\"\n",
    "\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "day made her feel very sleepy and stupid), whether the pleasure of\n",
    "making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\n",
    "There was nothing so very remarkable in that, nor did Alice think it so\n",
    "very much out of the way to hear the Rabbit say to itself, \"Oh dear! Oh\n",
    "dear! I shall be too late!\" But when the Rabbit actually took a watch\n",
    "out of its waistcoat-pocket and looked at it and then hurried on, Alice\n",
    "started to her feet, for it flashed across her mind that she had never\n",
    "before seen a rabbit with either a waistcoat-pocket, or a watch to take\n",
    "out of it, and, burning with curiosity, she ran across the field after\n",
    "it and was just in time to see it pop down a large rabbit-hole, under\n",
    "the hedge. In another moment, down went Alice after it!\n",
    "\"\"\"\n",
    "\n",
    "# Run the parser\n",
    "parser.add_pipe(parser.create_pipe('sentencizer'))     # for latest spacy 2.0.x, sentencizer needs to be added.\n",
    "parsedData = parser(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print sentences and tokens\n",
    "\n",
    "The result splits out sentences and tokens within sentences. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sentences = list(parsedData.sents)\n",
    "for i, s in enumerate(sentences):\n",
    "    tokens = [t.text for t in s if t.text.strip()]\n",
    "    print(i, ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print linguistic analysis\n",
    "\n",
    "The result also includes lemmas, part-of-speech (POS) tags, entity tags and grammatical dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('-'*79)\n",
    "print('{:<15} {:<15} {:<15} {:<15} {:<15}'.format('TOKEN', 'LEMMA', 'POS', 'ENTITY', 'DEPENDENCY'))\n",
    "print('-'*79)\n",
    "for token in sentences[0]:\n",
    "    if token.text.strip():\n",
    "        ent_tag = token.ent_iob_\n",
    "        if token.ent_type_:\n",
    "            ent_tag = '{}-{}'.format(token.ent_iob_, token.ent_type_)\n",
    "        dep_rel = token.dep_\n",
    "        if token.dep_ != 'ROOT':\n",
    "            dep_rel = '{}::{}'.format(token.dep_, token.head.text)\n",
    "        print('{:<15} {:<15} {:<15} {:<15} {:<15}'.format(token.text, token.lemma_, token.pos_, ent_tag, dep_rel))\n",
    "print('-'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Using parse data\n",
    "\n",
    "- Copy and paste the raw text for the first sentence into SpaCy's online demonstrator at https://spacy.io/demos/displacy. This gives a visual representation of the dependency graph.\n",
    "- Parse some news text (e.g., http://www.bbc.com/news/world-asia-china-36309063). Alternatively, view a parse using the demonstrator (e.g., https://spacy.io/demos/displacy?share=5036304037475277146). How could we use dependency relations to extract relationships from this data (e.g., `chairman-of<\"Mr Zhang\", \"National People's Congress Standing Committee\">`)?\n",
    "- How could we use SpaCY to include more linguistic analysis in our text classification and forecasting systems? If you have time, try some of these ideas (e.g., https://www.site.uottawa.ca/eng/school/publications/techrep/2007/TR-2007-12.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA2901 EXERCISE: Forecasting movie gross with support vector regression\n",
    "\n",
    "Imagine you have an opportunity to invest in films after seeing descriptions. Perhaps you run a cinema and need to decide which films to show. If we could predict box office gross based on descriptions, then we'd have a good basis for investment decisions.\n",
    "\n",
    "In this exercise we'll predict movie gross with support vector regression. \n",
    "\n",
    "### Download reviews and movie gross data\n",
    "\n",
    "DBPedia converts Wikipedia pages into structured semantic web data. Let's use it to grab the data we need. DBpedia has a public SPARQL endpoint at http://dbpedia.org/sparql. Enter the following query and save as `CSV` under `Results Format` and click `Run Query`.\n",
    "\n",
    "#### Query\n",
    "```\n",
    "PREFIX category: <http://dbpedia.org/resource/Category:>\n",
    "PREFIX dbtype: <http://dbpedia.org/datatype/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?dburl ?title ?budget ?gross ?abstract\n",
    "WHERE { \n",
    " ?dburl dcterms:subject category:2013_films .\n",
    "\n",
    " ?dburl foaf:name ?title .\n",
    " FILTER(LANG(?title) = \"en\") .\n",
    "\n",
    " ?dburl dbo:budget ?budget .\n",
    " FILTER(xsd:float(?budget) > xsd:float(\"1.0E7\")) .\n",
    " FILTER(datatype(?budget) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:gross ?gross .\n",
    " FILTER(datatype(?gross) = dbtype:usDollar) .\n",
    "\n",
    " ?dburl dbo:abstract ?abstract .\n",
    " FILTER(LANG(?abstract) = \"en\") .\n",
    " FILTER(fn:string-length(?abstract) >= xsd:int(140)) .\n",
    "} \n",
    "```\n",
    "\n",
    "This will return the box office gross and the Wikipedia abstract for English films from 2013 that had a USD budget of more than 10 million. This will be our training data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2013.csv`.\n",
    "\n",
    "Run the same query with `category:2014_films` to get test data. Upload to your `data` directory on Jupyter Hub and rename to `movies.2014.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "TRAIN_NAME = 'movies.2013.csv'\n",
    "TEST_NAME = 'movies.2014.csv'\n",
    "\n",
    "def read_movies(path):\n",
    "    data = []\n",
    "    target = []\n",
    "    for d in csv.DictReader(open(path, encoding=\"utf8\")):\n",
    "        data.append(d['abstract'])\n",
    "        target.append(float(d['gross']))\n",
    "    return data, target\n",
    "\n",
    "train_path = os.path.join('data', TRAIN_NAME)\n",
    "\n",
    "X_train, y_train = read_movies(train_path)\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "test_path = os.path.join('data', TEST_NAME)\n",
    "X_test, y_test = read_movies(test_path)\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters for support vector regression\n",
    "\n",
    "Let's see whether the results in [Kogan et al. (2009)](http://www.cs.cmu.edu/~nasmith/papers/kogan+levin+routledge+sagi+smith.naacl09.pdf) hold for the movie gross data we're using here.\n",
    "\n",
    "Specifically we will choose the best feature representation for both:\n",
    "* binary vs term frequency vs tfidf\n",
    "* unigrams vs bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "svr = Pipeline([('vect', CountVectorizer(lowercase=True)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVR(epsilon=0.1))\n",
    "               ])\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__binary': [True, False],\n",
    "              'tfidf__use_idf': [True, False],\n",
    "              'tfidf__sublinear_tf': [True, False],\n",
    "              'clf__C': [1e8, 1e9, 1e10],\n",
    "             }\n",
    "\n",
    "# Find best parameters\n",
    "gs_svr = GridSearchCV(svr, param_grid)\n",
    "gs_svr.fit(X_train, y_train)\n",
    "print('Grid search mean and stdev:\\n')\n",
    "for params, mean_score, scores in gs_svr.grid_scores_:\n",
    "    print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    "            mean_score, scores.std() * 2, params))\n",
    "print('\\nSVR best params:\\n', gs_svr.best_params_)\n",
    "print('\\nSVR r-squared on training data:\\n', gs_svr.score(X_train, y_train))\n",
    "print('\\nSVR r-squared on test data:\\n', gs_svr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Evaluation and discussion\n",
    "\n",
    "- According to the 95% prediction interval, how close will our predictions be to the actual value? What if we calculate over the test data instead?\n",
    "- How could we improve this experimental setup? We can use `gs_svr.steps` to access feature names (`vect.get_feature_names()`) and weights (`clf.coef_`) from the pipeline components respectively. Use Python `zip` function to combine and sort these. What do the highest-weighted features tell us about our experimental setup?\n",
    "- Would you use this model to pick investments?\n",
    "- Maybe we're not predicting the right thing. What derived values could we predict? If you have time to try them, are they better?\n",
    "- Is there other text data that might be a better predictor of box office returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
